\chapter{研究问题简述}
在本章节旨在对图像修复问题进行具体的简述，以及说明本文研究的具体问题。首先结合已有文献，对图像去噪问题的背景进行回顾以及定义必要的符号说明。
\section{背景回顾}
本文的主要研究问题为对目标图像进行去噪处理，得到相应去噪前的可能图像的生成。首先回顾传统生成模型的适用条件与背景。传统的扩散模型，如\cite{song_2,DDPM,DDIM}中的扩散模型，均用于实现从纯噪声图片生成原图像分布。其中最关键的步骤为Score-matching, 即为通过参数拟合学习得到真实的对数密度函数的梯度。即假设原定未知目标分布为$X_0\sim p(X_0)$, 通过构造扩散过程得到随机过程$X_{0:T}$使得$p(X_{T})\sim \mathcal{N}(0,I)$（或者使得分布趋近于标准正态分布即可）。 Score-matching即为，在选取较大的$T$的情况下，学到$\nabla_{X_t}\log (p(X_t))$的值，记该函数为Score-function。     

在\cite{song_2}中主要采用解逆向随机微分方程来获得对目标未知分布的一个采样。问题设定主要如下：   

目标未知分布为$p(x_0)$，通过构造扩散过程
\begin{equation}
    dx = f(x,t)dt + g(t)dW_t,
    \label{diffusion equation SDE 1}
\end{equation}
其中$W_t$为标准的布朗运动，以$x_0$为初始随机变量，实际通过欧拉法离散化生成$x_{i/T}$, $i=1,\cdots, T$, 最后采样得到$x_1$。通过设计特定形式的随机微分方程(\ref{diffusion equation SDE 1}),可以使得最终随机变量$x_1$的概率分布为形式较为简单的概率分布 (例如为标准高斯分布)。   

在已知(\ref{diffusion equation SDE 1}) 的形式的情形下，存在以$x_1$为初始值的逆向随机微分方程
\begin{equation}
     dx = \left(f(x,t)-g^2(t)\nabla_{x_t}\log(q_t(x_t))\right)dt + g(t)d\Tilde{w}_t,
    \label{reverse diffusion equation SDE 1}
\end{equation}
因此只需要得到打分函数(Score function)的信息，即可从$x_1$开始逆向利用欧拉法进行离散化取样，当步长$T$足够大的时候，则离散化间距$1/T$足够小, 根据 
\cite{detemple}中对于Monte Carlo Estimator的渐进性质的分析，在使用Euler 法进行离散化取样的时候可以以$1/\sqrt{T}$的收敛速度趋近真实随机变量的分布；当采用Doss Transformation使得随机变量的随机微分方程的Diffusion 项为非随机量的时候，则可以以$1/T$的速率分布上收敛至真实分布。考虑到(\ref{diffusion equation SDE 1})的形式，通常取$T=1000$量级的步长已经可以有较好的逼近效果。      

由于在(\ref{reverse diffusion equation SDE 1})中唯一的位置量即为目标函数Score function $\nabla_{x_t}\log(q_t(x_t))$, 可以用一个和时间和随机变量本身相关的函数$S_{\theta}(t,x)$ 来逼近，其中$\theta$为该函数的参数化。具体优化参数的方式即为
\begin{equation}
{\theta}^*=\underset{{\theta}}{\arg \min } \mathbb{E}_t\left\{\lambda(t) \mathbb{E}_{\mathbf{x}(0)} \mathbb{E}_{\mathbf{x}(t) \mid \mathbf{x}(0)}\left[\left\|\mathbf{s}_{{\theta}}(\mathbf{x}(t), t)-\nabla_{\mathbf{x}(t)} \log p_{0 t}(\mathbf{x}(t) \mid \mathbf{x}(0))\right\|_2^2\right]\right\},
\label{score matching}
\end{equation}
其中$\lambda(t)$, $t\in [0,1]$为设置的权重函数，旨在使得优化得到的权重函数不仅仅在优化某些特定时刻的误差。优化得到最佳的打分函数Score function 的优化过程 (\ref{score matching}) 即为Score matching。    


根据\cite{score_based_SDE}中通过解逆向随机微分方程的想法，可以通过设计不同的随机微分方程形式来得到不同的采样方式，以及可以设计不同的正向采样和逆向采样算法，可以让生成方式为Markovian的也可以为Non-Markovian的。在\cite{DDPM}中设计了一种特定的随机微分方程来进行采样生成，方程满足如下形式。
\begin{equation}
    dz = -\left(\beta(t)/2\right)z dt + \sqrt{\beta(t)}dW_t,
    \label{DDPM Equation}
\end{equation}
在进行离散化取样生成正向传播链的时候，可以表达为目标分布为$x_0$，从$x_0$开始正向传播生成$x_{1:T}$, 满足条件概率分布为
\begin{equation}
    q\left(x_{1: T} \mid x_0\right)=\prod_{t=1}^T q\left(x_t \mid x_{t-1}\right)
    \end{equation}
    \begin{equation}
        q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(\sqrt{1-\beta_t} x_{t-1}, \beta_t I\right)
        \end{equation}
        \begin{equation}
            q\left(x_t \mid x_0\right)=\mathcal{N}\left(\sqrt{\bar{\alpha}_t} x_0,\left(1-\overline{\alpha_t}\right) I\right) \text { 其中 } \alpha_t=\left(1-\beta_t\right) \text { 以及 } \bar{\alpha}_t=\prod_t \alpha_t
            \label{posterior of xt}
            \end{equation}
    正向传播的后验分布同样可以被给出：
    \begin{equation}
        q\left(x_{t-1} \mid x_t, x_0\right)=\mathcal{N}\left(\tilde{\mu}_t\left(x_t, x_0\right), \tilde{\beta}_t\right)
        \label{posterior xt 2}
        \end{equation}
        \begin{equation}
            \text {     其中 } \tilde{\mu}_t\left(x_t, x_0\right)=\frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1-\bar{\alpha}_t} x_0+\frac{\sqrt{\alpha_t}\left(1-\bar{\alpha}_{t-1}\right)}{1-\bar{\alpha}_t} x_t
            \end{equation}
            \begin{equation}
                \text { 以及 } \quad \tilde{\beta}_t=\frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t
                \end{equation}
该正向过程实际上对应的随机微分方程即为
\begin{equation}
    dz = -\frac{g(t)^2}{2}\cdot  z dt + g(t)dw
\end{equation}
$x_{0:T}$的联合分布$p_{\theta}(x_{0:T})$被称为逆向过程，并且它被定义为一个Markov过程，该过程的转移分布可以从末端$p(x_T)= \mathcal{N}(x_T;0,I) $开始。由此可以写出逆向过程的联合分布函数的表达式
\begin{equation}
    p_{\theta}(x_{0:T}) := p(x_T)\prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_t), p_{\theta}(x_{t-1}|x_t) :=\mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)).  
\end{equation}

以上为正向Markov过程所满足的条件概率分布性质，在随机优化问题中为了优化参数需要对逆向传播过程进行建模。
逆向传播过程同样可以被参数化，此处使用高斯转移分布来进行此一阶Markov逆向传播过程，满足如下表达式：
\begin{equation}
    p\left(x_{0: T}\right)=p\left(x_T\right) \prod_{t=1}^T p_\theta\left(x_{t-1} \mid x_t\right)
    \end{equation}
    \begin{equation}
        p_\theta\left(x_{t-1} \mid x_t\right)=\mathcal{N}\left(\mu_\theta\left(x_t, t\right), \Sigma_\theta\left(x_t, t\right)\right)
        \end{equation}
        % \begin{figure}[H]
        %     \centering
        %     \includegraphics[scale = 0.2]{ThuThesis_ Tsinghua University Thesis LaTeX Template/Picture/Diffusion.png}
        %     \caption{扩散模型图例}
        %     \label{Diffusion}
        % \end{figure}
选取足够大的$T$值与合适的递减序列$\{\beta_t\}_{t\geq 1}$， 条件概率分布$q(x_{T}|x_0)$可以足够接近标准高斯分布。
整个概率系统可以通过变分推断来进行端到端的训练，从而优化参数。
在该模型下，同样采用极大似然估计的方式来进行参数优化，此处采用最大化对数似然函数的相反数的上界来进行优化，即
\begin{equation}
    \mathbb{E}\left[-\log p_\theta\left(\mathbf{x}_0\right)\right] \leq \mathbb{E}_q\left[-\log \frac{p_\theta\left(\mathbf{x}_{0: T}\right)}{q\left(\mathbf{x}_{1: T} \mid \mathbf{x}_0\right)}\right]=\mathbb{E}_q\left[-\log p\left(\mathbf{x}_T\right)-\sum_{t \geq 1} \log \frac{p_\theta\left(\mathbf{x}_{t-1} \mid \mathbf{x}_t\right)}{q\left(\mathbf{x}_t \mid \mathbf{x}_{t-1}\right)}\right]=: L
    \label{DDPM objective 1}
\end{equation}
从随机优化方法的角度去考虑，可以将式(\ref{DDPM objective 1})重新写为
\begin{equation}
\mathbb{E}_q[\underbrace{\mathcal{D}_{K L}\left(q\left(x_T \mid x_0\right) \| p\left(x_T\right)\right)}_{L_T}+\sum_{t>1} \underbrace{\mathcal{D}_{K L}\left(q\left(x_{t-1} \mid x_t, x_0\right) \| p_\theta\left(x_{t-1} \mid x_t\right)\right)}_{L_{t-1}}-\underbrace{\log p_\theta\left(x_0 \mid x_1\right)}_{L_0}]
    \end{equation}
以上为DDPM的数学原理，以下将具体讨论DDPM在实际优化过程中的参数设计与模型细节。以上已经将$L$分解成$L_T,L_{t-1},L_0$三大部分，由于后验分布$q$没有任何参数，因此$L_T$可以直接忽略（和$\theta$取值近似无关）。
在\cite{DDPM}中直接假设$\Sigma_{\theta}(x_t,t)=\sigma_t^2 I$，方便简化计算过程（避免矩阵求逆运算操作）。其次，为了表示$\mu_{\theta}(x_t,t)$，由于$p_{\theta}(x_{t-1}|x_t):= \mathcal{N}(x_{t-1};\mu_{\theta}(x_t,t),\sigma_t^2I)$,我们可以将$L_{t-1}$写为
\begin{equation}
    L_{t-1}=\mathbb{E}_q\left[\frac{1}{2 \sigma_t^2}\left\|\tilde{{\mu}}_t\left(\mathbf{x}_t, \mathbf{x}_0\right)-{\mu}_\theta\left(\mathbf{x}_t, t\right)\right\|^2\right]+C
    \label{Lt representation}
\end{equation}
其中$C$为一个和$\theta$无关的常数，在优化过程中可以被忽略。利用式(\ref{posterior of xt})可以进一步简化式(\ref{Lt representation}) 将$x_t$表示为$\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right)=\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} {\epsilon} \text { for } {\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) $, 以及利用式(\ref{posterior xt 2})可以得到
\begin{align}
    L_{t-1}-C&=\mathbb{E}_{\mathbf{x}_0, {\epsilon}}\left[\frac{1}{2 \sigma_t^2}\left\|\tilde{{\mu}}_t\left(\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right), \frac{1}{\sqrt{\bar{\alpha}_t}}\left(\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right)-\sqrt{1-\bar{\alpha}_t} {\epsilon}\right)\right)-{\mu}_\theta\left(\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right), t\right)\right\|^2\right]\\
&=\mathbb{E}_{\mathbf{x}_0, {\epsilon}}\left[\frac{1}{2 \sigma_t^2}\left\|\frac{1}{\sqrt{\alpha_t}}\left(\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right)-\frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}} {\epsilon}\right)-{\mu}_\theta\left(\mathbf{x}_t\left(\mathbf{x}_0, {\epsilon}\right), t\right)\right\|^2\right]
\end{align}
经过\cite{DDPM,score_based_SDE}中的计算化简，\ref{DDPM objective 1}实际上可以化简为
\begin{equation}
    \mathbb{E}_{t\sim \mathcal{U}(0,1),\epsilon\sim \mathcal{N}(0,I)}\left[\|\epsilon - \epsilon^{\theta}(x_t,t)\|^2\right]
\end{equation}
而此时的打分函数满足$s^{\theta}(x_t,t) =- \frac{1}{\sqrt{\Bar{\beta}_t}}\epsilon^{\theta}(x_t,t)$,取 $\mu^{\theta}(x_t,t) = \frac{1}{\sqrt{\Bar{\alpha}_t}}\left(x_t- \sqrt{\Bar{\beta}_t}\epsilon^{\theta}(x_t,t)\right)$。 以上为DDPM主要的实现原理与优化过程。

在当下利用Diffusion Model实现图像去噪，图像修损任务中的研究中，主要采用DDPM模型中的随机微分方程进行向前扩散，以及采用和Langevin dynamic 中进行Score matching的方法进行混合使用提升收敛速率。但是实质并没有充分利用图像加噪和去噪随机变量之间的条件分布，本质上是在实现更高效率的生成式模型进行图像还原。下面进行前向加噪模型的一般形式阐述。

前向加噪模型可以总体以如下形式表达：
\begin{equation}
y=\mathcal{H}\left(x_0\right)+{n}, \quad y, {n} \in \mathbb{R}^n, x \in \mathbb{R}^d,
\label{Forward model}
\end{equation}
其中$x_0$为原始分布，即为初始图像采样样本，$y$为加过噪声处理后的图像，$\mathcal{H}$为加噪算符，而$n$表示随机噪声。首先仅考虑$n$为高斯噪声的情况，即$n\sim \mathcal{N}(0,\sigma^2 I_n)$。即$y$服从条件概率分布,$y\sim \mathcal{N}(\mathcal{H}(x_0)\mid \sigma^2 I_n)$。
进一步简化问题，以及考虑实际应用中的下游任务主要集中于降噪，图像修复，去模糊化处理，超分辨率化以及等情形，在此处可假设$\mathcal{H}(\cdot)$为线性变换（线性变换可以适用于图像平移，旋转，图像部分缺损等实际应用场景），对于较为复杂的加噪模型可以在未来的工作中用泛化能力更强的模型进行去噪处理。      
\begin{definition}[图像去噪]
在加噪模型（\ref{Forward model}）下，对于加噪后的图像$y$而言存在非唯一的原像。在已知去噪前图像分布数据集$p(x)$的分布和加噪图像后验分布$p(y\mid x)$的条件下，目标是得到$p(x)$内的一个子分布使得最大化后验分布$p(x\mid y)$，即采样得到原图像分布内最有可能经过加噪处理得到目标加噪图片的图像集合的分布。
\end{definition}

在图像去噪任务中，主要目标可以阐释为，在已知加噪后的图像$y$的情形下，还原得到最契合去噪前原始图像的图像分布$x$，即找到最合适的随机分布$x\sim p(x)$，使得
\begin{equation}
    x^{*} = \arg \max_{x\in \mathcal{X}}\log p( x\mid y),
\end{equation}
其中$\mathcal{X}$为可能的图像分布的取值集合空间，相比于在diffusion model中极大化似然函数$\log(p(x))$，此处相当于变成了条件概率分布意义下的似然函数。若同样设计DDPM中的随机微分方程作为向前扩散模型, 则只需要通过拟合条件概率分布意义下的打分函数Score function $\nabla_{x_t}\log\left(p(x_t\mid y)\right)$。   






对于噪声$n$的分布也可以设置一定的先验假设。如果仅考虑$n$为高斯噪声的情形，则条件似然函数有以下形式：
\begin{equation}
 p\left(y \mid x_0\right)=\frac{1}{\sqrt{(2 \pi)^n \sigma^{2 n}}} \exp \left[-\frac{\left\|y-\mathcal{H}\left(x_0\right)\right\|_2^2}{2 \sigma^2}\right],
 \label{log-likelihood gaussion }
\end{equation}
根据Bayes公式我们有
\begin{equation}
    p_t(x_t\mid y) = \frac{p_t(x_t)}{p_t(y)}\cdot p_t(y\mid x_t),
\end{equation}
用到$\nabla_{x_t}p_t(y)=0$, 因此可以化简得到如下结果
\begin{equation}
\nabla_{x_t} \log p_t\left(x_t \mid y\right)=\nabla_{x_t} \log p_t\left(x_t\right)+\nabla_{x_t} \log p_t\left(y \mid x_t\right).
\label{conditional score function }
\end{equation}
因此在本图像去噪任务中最重要的任务即为如何拟合函数$\log \left(p(y\mid x_t)\right)$。

该问题的困难之处在于，如果重新训练一个新的模型用来逼近该打分函数，计算量上的难度较大，以及会有较大的重构误差(reconstruction error)。因此需要引入新的方法来进行建模。在\cite{song_2}中采用先用$x_t$替代$x_0$再对条件概率分布下的方差进行手动调整的方法逼近打分函数，即
\begin{equation}
    \nabla_{x_t}\log\left(p(y\mid x_t)\right) \approx \nabla_{x_t}\left(-\frac{\|y-Hx_t\|^2}{2\left(\sigma_y^2+\gamma_t^2\right)}\right) = \frac{H^{\top}(Hx_t-y)}{\sigma_y^2+\gamma_t^2},
    \label{approx MRI}
\end{equation}
该方法的优点在于不需要重新训练模型，只需要（\ref{approx MRI}）中的具有解析形式的函数去逼近现有的条件概率分布下的Score function （\ref{conditional score function }），而$\nabla_{x_t}\log\left(p(x_t)\right)$可以直接使用预训练的模型进行使用，因此该方法较为简便在计算效率上具有高效性。缺点在于超参数$\{\gamma_t\}_{t\geq 0}$需要进行人为选择，以及直接使用$x_t$替代$x_0$， 对于真实的概率分布的逼近效果没有理论上的保证。



我们的总体思路如下，根据原始图像$x_0$与给定的随机微分方程\ref{diffusion equation SDE 1}得到随机过程$\{x_t\}_{t\in [0,T]}$, 使得$x_T$为分布可知的随机变量。 再利用极大似然估计的方法极大化$\log\left(p_{\theta}(x_0\mid y)\right)$ 从而得到相应的逆向过程。核心即为，在极大化条件似然函数的情况下，得到相应新的打分函数$\nabla_{x_t}\log(q_t(x_t\mid y))$。从而可以通过逆向随机微分方程得到相应的随机变量分布进行采样。

借鉴\cite{DDPM}中的原理，可以选取形式较为简单的随机微分方程形式来实现扩散模型。例如在VE SDE(Variance Exploding SDE)下，选取$f(x,t)=0$以及$g(t) = \sqrt{\frac{d}{dt}\sigma^2(t)}$，则相邻两个随机变量的采样过程可以表示为
\begin{equation}
    x_{(i+1)} = x_{i}+\sqrt{\sigma_i^2-\sigma_{i-1}^2}z_{i-1}, z_{i-1}\sim \mathcal{N}(0,I)
\end{equation}


在该条件下, 前向传播条件密度函数 
\begin{equation}
  q\left(x_t \mid x_{t-1}\right)=\mathcal{N}\left(x_{t-1},\left(\sigma_t^2-\sigma_{t-1}^2\right) I\right), q\left(x_t \mid x_0\right)=\mathcal{N}\left(x_0, \sigma_t^2 I\right),  
\end{equation}
, 以及我们还有
\begin{equation}
    q\left(x_{t-1} \mid x_t, x_0\right)=\mathcal{N}\left(\frac{\sigma_{t-1}^2}{\sigma_t^2} x_t+\left(1-\frac{\sigma_{t-1}^2}{\sigma_t^2}\right) x_0, \frac{\sigma_{t-1}^2}{\sigma_t^2}\left(\sigma_t^2-\sigma_{t-1}^2\right) l\right) .
\end{equation}
在进行非去噪任务，单纯进行非条件生成的时候，逆向生成可以采用如下方式：
\begin{equation}
    x_{t-1}=\frac{\sigma_{t-1}^2}{\sigma_t^2} x_t+\left(1-\frac{\sigma_{t-1}^2}{\sigma_t^2}\right) \hat{x}_0\left(x_t\right)+\frac{\sigma_{t-1}}{\sigma_t} \sqrt{\sigma_t^2-\sigma_{t-1}^2} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
\end{equation}
其中我们有$\hat{x}_0\left(x_t\right)=\mathbb{E}\left[x_0 \mid x_t\right]=x_t+\sigma_t^2 \nabla_{x_t} \log p_t\left(x_t\right)$, 则我们可以将如上生成公式写成如下形式
\begin{equation}
    x_{t-1}=x_t+\left(\sigma_t^2-\sigma_{t-1}^2\right) s_\theta\left(x_t, \sigma_t\right)+\frac{\sigma_{t-1}}{\sigma_t} \sqrt{\sigma_t^2-\sigma_{t-1}^2} \epsilon=: f\left(x_t ; t\right), \quad \epsilon \sim \mathcal{N}(0, I).
    \label{reverse sampling}
\end{equation}
在\cite{song_2,DDPM}中主要运用学习到的Score function来进行逆向采样得到对目标分布的逼近。在条件生成任务中，类比无条件生成中的情形，可以利用相似的原理对条件概率分布下的Score function来进行逆向分布。在\cite{Inverse}中通过对$p(x_0\mid x_t)$的逼近来得到对$p(y\mid x_t)$的逼近，从而可以实现逆向采样。
同样，在\cite{Inverse}中考察了在选取VP-SDE（Variance Preserving SDE）下的采样过程，主要如下：      

1. 先通过$x_t$得到$\hat{x}_0$，即
\begin{equation}
    \hat{{x}}_0 \leftarrow \frac{1}{\sqrt{\bar{\alpha}_t}}\left({x}_t+\left(1-\bar{\alpha}_t\right) \hat{{s}}(x_t,t)\right).
\end{equation}
        
        2.再生成白噪声用于后续生成，即生成噪声$z$,
\begin{equation}
    {z} \sim \mathcal{N}(\mathbf{0}, {I}) 
\end{equation}
      
      
      3. 最后再根据DDPM下的采样方法进行条件采样
\begin{equation}
     {x}_{i-1}^{\prime} \leftarrow \frac{\sqrt{\alpha_i}\left(1-\bar{\alpha}_{i-1}\right)}{1-\bar{\alpha}_i} {x}_i+\frac{\sqrt{\bar{\alpha}_{i-1}} \beta_i}{1-\bar{\alpha}_i} \hat{{x}}_0+\tilde{\sigma}_i {z}
\end{equation}


以及在DDPM设置的模型下，有如下两个结论可以帮助逼近$p(y\mid x_t)$：

\begin{proposition}
\label{prop 1}
     在使用DDPM条件下的随机微分方程进行向前采样的扩散模型下, $p\left(x_0 \mid x_t\right)$ 有唯一的后验概率分布下的数学期望
$$
\hat{x}_0:=\mathbb{E}\left[x_0 \mid x_t\right]=\frac{1}{\sqrt{\bar{\alpha}(t)}}\left(x_t+(1-\bar{\alpha}(t)) \nabla_{x_t} \log p_t\left(x_t\right)\right)
$$
\end{proposition}
\begin{theorem}
\label{thm 1}
对于给定的线性加噪模型(\ref{Forward model}) 满足噪声分布${n} \sim \mathcal{N}\left(0, \sigma^2 {I}\right)$，我们有如下渐进分析结果
\begin{equation}
  p\left(y \mid x_t\right) \approx p\left(y \mid \hat{x}_0\right),
  \label{Thm 1 eq 1}
\end{equation}

其中逼近误差可以利用詹森误差\footnote{詹森误差定义：$\mathcal{J}(f,x\sim p(x)) = \mathbb{E}[f(x)]-f(\mathbb{E}[x])$, 其中数学期望是在服从$p(x)$概率分布意义下的数学期望}来表示，其上界为
\begin{equation}
\mathcal{J} \leq \frac{d}{\sqrt{2 \pi \sigma^2}} e^{-1 / 2 \sigma^2}\left\|\nabla_{x} \mathcal{H}(x)\right\| m_1,
    \label{upper bound thm 1}
\end{equation}
其中我们还有
\begin{equation}
    \left\|\nabla_{x} \mathcal{H}(x)\right\|:=\max _{x}\left\|\nabla_{x} \mathcal{H}(x)\right\|,
\end{equation}
以及
\begin{equation}
m_1:=\int_{\mathbb{R}^d}\left\|x_0-\hat{x}_0\right\| p\left(x_0 \mid x_t\right) d x_0  
\end{equation}
\end{theorem}
根据\ref{prop 1}和\ref{thm 1}的结论，可以通过$p(y\mid \hat{x}_0(x_t))$来得到对与$p(y\mid x_t)$
的逼近。在\cite{Inverse}中，在每一步逆向采样中，主要分为两步：   

（i）首先根据在无条件生成情况下，根据Diffusion Model的结构，通过(\ref{reverse sampling})进行一步逆向采样,通过$x_i$得到$x_{i-1}^{\prime}$。    

（ii）再通过估计$\nabla_{x_t}\log(p(y\mid x_t))$，通过$x_{i-1}^{\prime}$得到$x_{i-1}$，具体更新形式为
\begin{equation}
    x_{i-1} \leftarrow x_{i-1}^{\prime}-\zeta_i \nabla_{x_i}\left\|y-\mathcal{H}\left(\hat{x}_0\right)\right\|_2^2, \label{update 2 DPS}
\end{equation}
其中$\zeta_i$为预先设定的步长。

在\cite{Inverse}提出的条件生成算法中，只需要前向加噪满足自动微分条件，即可以得到$\nabla_x\mathcal{H}(x)$，则可以对无论线性或者非线性加噪模型均进行图像修复任务。相比于传统图像修复任务中对于$\mathcal{H}$均假设为线性有了较大的创新性和适用范围，以及不需要获取过多对于$\mathcal{H}$的信息。     

但是该算法的局限性在于      


1. 需要手动设置步长$\{\zeta_i\}_{i\geq 1}$，对于不同的数据集选用的相应取值需要进行调整。 而步长实际对应为后验概率分布下的方差，即$\operatorname{Var}\left[y\mid x_t\right]$，在\cite{song2023pseudoinverse,Inverse}中均采用选取对其估计值的方法人工指定步长，会带来一定的误差。   


2. 使用DDPM模型作为基准扩散模型进行反向采样，但是采样过程中需要对整条链进行生成。若采用DDIM等模型进行替代，结合\cite{DDIM}，则可以在不需要生成整条链的情况下进行图像生成，相比于原先的情形有了较大的效率上的提升。   

3. 以及噪声预测神经网络结构中只使用了在\cite{Unet}中提出的Unet神经网络结构，可以结合在\cite{DDIM,VAE_diffusion}中所提出结合transformer模型所利用的attention机制，进一步提升模型性能。     

4.在\cite{Inverse}中所提出的方法目前仅仅在FFHQ10数据集上得到验证，在如Imagenet等数据集上的具体效果有待后续进行测试。    

根据以上局限性，后续可以进行如下工作
\begin{itemize}
    \item 构造DDIM模型下的条件生成任务，结合\cite{song2023pseudoinverse}中提出的Pseudoinverse 算法，在\cite{Inverse}基础上进一步增强生成效率.
    \item  尝试改善在\cite{Inverse}中较为随意的$\{\zeta_i\}_{i\geq 1}$方式，得到更加精确的后验方差估计，可结合在\cite{Analytic_DPM}中对model-free 假设下最佳后验估计的结果。
    \item 尝试在不同数据集下使用DPS算法的生成效果，目前的DPS算法仅在FFHQ10数据集下进行实现。    
\end{itemize}
本文将在DPS算法\cite{Inverse}的基础上进行改进，获得对真实后验分布$p(y\mid x_t)$的更高精度逼近。